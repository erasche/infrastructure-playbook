---

host_authorized_key_users: "{{ vault_host_authorized_key_users }}"
host_private_key_users: "{{ vault_host_private_key_users }}"

# legacy mounts are used for these two datasets to ensure proper mounting order
mounts:
  - src: promise-1/orval/depot
    path: /srv/nginx/depot.galaxyproject.org/root
    fstype: zfs
    state: mounted
    backup: yes
    owner: depot
    group: depot
  - src: ift-1/orval/depot/singularity
    path: /srv/nginx/depot.galaxyproject.org/root/singularity
    fstype: zfs
    state: mounted
    backup: yes
    owner: singularity
    group: singularity

host_packages:
  - subversion  # for bioarchive

# uids and gids were auto-generated on old orval (except nate's) which is the
# reason for the uid/gid mess

host_users:
  - name: sites
    uid: 1000
    gid: 2049
    group: sites
    comment: 'Web sites'
  - name: bioarchive
    uid: 2049
    gid: 2050
    group: bioarchive
    comment: 'bioaRchive'
  - name: depot
    uid: 2052
    gid: 2053
    group: depot
    comment: 'Galaxy Depot'
  - name: bag
    uid: 1001
    gid: 1000
    group: bag
    comment: Björn Grüning
    home: /home/bag
    shell: /bin/bash
  - name: esr
    uid: 2050
    gid: 2051
    group: esr
    comment: Helena Rasche
    home: /home/erasche
    shell: /bin/bash
  - name: nate
    uid: 2048
    gid: 2048
    group: nate
    comment: Nate Coraor
    home: /home/nate
    shell: /bin/bash
  - name: zfsdump-orval
    uid: 2051
    gid: 2052
    group: zfsdump-orval
    comment: 'ZFS send orval'
    home: /home/zfsdump-orval
    shell: /bin/bash
  - name: singularity
    uid: 2054
    gid: 2054
    group: singularity
    home: /home/singularity
    shell: /bin/bash
  - name: training
    uid: 2055
    gid: 2055
    group: training
    home: /home/training
    shell: /bin/bash
  - name: starforge
    uid: 2056
    gid: 2056
    group: starforge
    home: /home/starforge
    shell: /bin/bash

zfs_filesystems:
  - name: promise-1/orval/vicepa
    mountpoint: /vicepa
    atime: "off"
  - name: promise-1/orval/snpeff-data
    mountpoint: /srv/nginx/snpeff-data.galaxyproject.org/root
    exec: "off"
    setuid: "off"
    owner: bag
    group: bag
  - name: promise-1/orval/bioarchive
    mountpoint: /srv/nginx/bioarchive.galaxyproject.org/root
    exec: "off"
    setuid: "off"
    owner: bioarchive
    group: bioarchive
  - name: promise-1/orval/images
    mountpoint: /srv/nginx/images.galaxyproject.org/root
    exec: "off"
    setuid: "off"
    owner: sites
    group: sites
  - name: promise-1/orval/sample-data
    mountpoint: /srv/nginx/sample-data.galaxyproject.org/root
    exec: "off"
    setuid: "off"
    owner: sites
    group: sites
  - name: promise-1/orval/wheels
    mountpoint: /srv/nginx/wheels.galaxyproject.org/root
    exec: "off"
    setuid: "off"
    owner: sites
    group: sites
  - name: promise-1/orval/screencast
    mountpoint: /srv/nginx/screencast.galaxyproject.org/root
    exec: "off"
    setuid: "off"
    owner: sites
    group: sites
  - name: promise-1/orval/depot
    mountpoint: legacy
    exec: "off"
    setuid: "off"
    owner: depot
    group: depot
  - name: ift-1/orval/depot/singularity
    mountpoint: legacy
    exec: "off"
    setuid: "off"
    owner: singularity
    group: singularity

host_directories:
  - path: /srv/nginx/depot.galaxyproject.org/root/training
    owner: training
    group: training
  - path: /srv/nginx/depot.galaxyproject.org/root/starforge
    owner: starforge
    group: starforge
  - path: /srv/nginx/depot.galaxyproject.org/root/starforge/travis
    owner: starforge
    group: starforge

host_templates:
  - dest: /etc/rsyncd.conf
    src: ini.j2
    contents:
      software:
        path: /srv/nginx/depot.galaxyproject.org/root/software
        comment: Community Package Cache
        use chroot: 'yes'
        uid: 'nobody'
        gid: 'nogroup'
        read only: 'yes'
        list: 'yes'
        dont compress: '*.tgz *.zip *.gz *.bz2 *.bam *.2bit *.lzo'
      singularity:
        path: /srv/nginx/depot.galaxyproject.org/root/singularity
        comment: BioContainers Singularity Images
        use chroot: 'yes'
        uid: 'nobody'
        gid: 'nogroup'
        read only: 'yes'
        list: 'yes'
        dont compress: '*.tgz *.zip *.gz *.bz2 *.bam *.2bit *.lzo'

host_service_configurations:
  - src: files/orval/rsyncd.xml

host_services:
  - name: network/rsyncd
    state: started
    enabled: 'yes'

zfs_admin_permissions:
  - name: promise-1/orval/vicepa
    users: zfsdump-orval
    permissions: send,snapshot,hold
  - name: promise-1/orval/snpeff-data
    users: zfsdump-orval
    permissions: send,snapshot,hold
  - name: promise-1/orval/bioarchive
    users: zfsdump-orval
    permissions: send,snapshot,hold
  - name: promise-1/orval/images
    users: zfsdump-orval
    permissions: send,snapshot,hold
  - name: promise-1/orval/sample-data
    users: zfsdump-orval
    permissions: send,snapshot,hold
  - name: promise-1/orval/wheels
    users: zfsdump-orval
    permissions: send,snapshot,hold
  - name: promise-1/orval/screencast
    users: zfsdump-orval
    permissions: send,snapshot,hold
  - name: promise-1/orval/depot
    users: zfsdump-orval
    permissions: send,snapshot,hold

host_crontabs:
  - name: "Backup OpenAFS /vicepa"
    id: backup_vicepa
    user: zfsdump-orval
    hour: 0
    minute: 0
    job: dataset='promise-1/orval/vicepa' && backupserver='westvleteren.galaxyproject.org' && {{ zfs_backup_job_template }}
  - name: "Backup SnpEff data"
    id: backup_snpeff_data
    user: zfsdump-orval
    hour: 0
    minute: 30
    job: dataset='promise-1/orval/snpeff-data' && backupserver='westvleteren.galaxyproject.org' && {{ zfs_backup_job_template }}
  - name: "Backup BioaRchive"
    id: backup_bioarchive
    user: zfsdump-orval
    hour: 0
    minute: 45
    job: dataset='promise-1/orval/bioarchive' && backupserver='westvleteren.galaxyproject.org' && {{ zfs_backup_job_template }}
  - name: "Backup images"
    id: backup_images
    user: zfsdump-orval
    hour: 1
    minute: 0
    job: dataset='promise-1/orval/images' && backupserver='westvleteren.galaxyproject.org' && {{ zfs_backup_job_template }}
  - name: "Backup sample data"
    id: backup_sample_data
    user: zfsdump-orval
    hour: 1
    minute: 15
    job: dataset='promise-1/orval/sample-data' && backupserver='westvleteren.galaxyproject.org' && {{ zfs_backup_job_template }}
  - name: "Backup screencasts"
    id: backup_screencast
    user: zfsdump-orval
    hour: 1
    minute: 45
    job: dataset='promise-1/orval/screencast' && backupserver='westvleteren.galaxyproject.org' && {{ zfs_backup_job_template }}
  - name: "Backup depot"
    id: backup_depot
    user: zfsdump-orval
    hour: 2
    minute: 0
    job: dataset='promise-1/orval/depot' && backupserver='westvleteren.galaxyproject.org' && {{ zfs_backup_job_template }}
  - name: "Backup depot/singularity"
    id: backup_depot_singularity
    user: zfsdump-orval
    hour: 2
    minute: 15
    job: dataset='ift-1/orval/depot/singularity' && backupserver='westvleteren.galaxyproject.org' && {{ zfs_backup_job_template }}

nginx_conf_http:
  server_names_hash_bucket_size: 64

# TODO: disable 1.0, 1.1
nginx_conf_ssl_protocols:
  - TLSv1
  - TLSv1.1
  - TLSv1.2

nginx_servers:
  - certbot_redirects

nginx_ssl_servers:
  - orval
  - snpeff-data
  - screencast
  - images
  - sample-data
  - bioarchive
  - depot

# needs cffi, which needs gcc, which isn't installed
#certbot_virtualenv_package_name: py37-virtualenv
certbot_install_method: package
certbot_package: py37-certbot
certbot_script: /opt/local/bin/certbot
certbot_letsencrypt_dir: /opt/local/etc/letsencrypt
certbot_post_renewal: |
  svcadm restart nginx || true
certbot_domains:
  - "{{ inventory_hostname }}"
  - snpeff-data.galaxyproject.org
  - screencast.galaxyproject.org
  - screencast.g2.bx.psu.edu
  - images.galaxyproject.org
  - sample-data.galaxyproject.org
  - bioarchive.galaxyproject.org
  - depot.galaxyproject.org
